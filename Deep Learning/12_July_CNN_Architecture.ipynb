{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Describe the purpose and benefits of pooling in CNN.**\n",
        "\n",
        "**Ans 1:**\n",
        "\n",
        "**a. Purpose and Benefits:**\n",
        "\n",
        "**Pooling in Convolutional Neural Networks (CNNs) serves the following purposes with associated benefits:**\n",
        "\n",
        "**Purpose:**\n",
        "   - **Down-Sampling:**\n",
        "      - **Benefit:** Pooling reduces the spatial dimensions of the input data, down-sampling the feature maps.\n",
        "      - **Advantage:** This downsizing is crucial for focusing on essential information and decreasing the computational load in subsequent layers.\n",
        "\n",
        "**Benefits:**\n",
        "   - **Feature Reduction:**\n",
        "      - **Benefit:** Pooling retains the most relevant features while discarding less important details.\n",
        "      - **Advantage:** This feature reduction simplifies the representation of the data, making it computationally efficient.\n",
        "\n",
        "   - **Translation Invariance:**\n",
        "      - **Benefit:** Pooling introduces a degree of translation invariance, making the model less sensitive to small variations in the position of features.\n",
        "      - **Advantage:** This is particularly useful when the exact location of a feature is less important than its presence.\n",
        "\n",
        "   - **Increased Receptive Field:**\n",
        "      - **Benefit:** Pooling increases the receptive field of the network, allowing it to capture more global information.\n",
        "      - **Advantage:** The network can learn higher-level abstractions by considering larger regions of the input.\n",
        "\n",
        "**b. Summary:**\n",
        "   - Pooling, through down-sampling and feature reduction, contributes to the efficiency, interpretability, and robustness of CNNs by focusing on essential information while maintaining translation invariance and an increased receptive field.\n",
        "\n",
        "---\n",
        "\n",
        "**Q2. Explain the difference between min pooling and max pooling.**\n",
        "\n",
        "**Ans 2:**\n",
        "\n",
        "**a. Min Pooling:**\n",
        "   - **Operation:** In min pooling, the minimum value from a group of neighboring pixels is selected.\n",
        "   - **Characteristics:** It highlights the smallest feature within the pooling region.\n",
        "   - **Use Case:** Min pooling may be suitable when the goal is to emphasize the least intense features in an image.\n",
        "\n",
        "**b. Max Pooling:**\n",
        "   - **Operation:** In max pooling, the maximum value from a group of neighboring pixels is chosen.\n",
        "   - **Characteristics:** It emphasizes the most prominent feature within the pooling region.\n",
        "   - **Use Case:** Max pooling is commonly used for highlighting the most significant features, such as edges or patterns.\n",
        "\n",
        "**c. Difference:**\n",
        "   - **Selection Criterion:** The key difference lies in the selection criterionâ€”min pooling selects the minimum value, while max pooling selects the maximum.\n",
        "   - **Effect on Features:** Min pooling may preserve less intense features, while max pooling emphasizes the most intense features.\n",
        "\n",
        "**d. Summary:**\n",
        "   - Min pooling and max pooling are operations used in CNNs to down-sample feature maps. They differ in their selection criteria, affecting which features are highlighted in the down-sampled representation.\n",
        "\n",
        "---\n",
        "\n",
        "**Q3. Discuss the concept of padding in CNN and its significance.**\n",
        "\n",
        "**Ans 3:**\n",
        "\n",
        "**a. Concept of Padding:**\n",
        "   - **Definition:** Padding involves adding extra pixels around the input image or feature map.\n",
        "   - **Significance:** Padding addresses the issue of information loss at the edges of an image during convolution.\n",
        "\n",
        "**b. Significance:**\n",
        "   - **Edge Preservation:** Without padding, the pixels at the edges of the input receive fewer convolutions, leading to potential information loss.\n",
        "   - **Boundary Effects Mitigation:** Padding mitigates boundary effects, ensuring that the network can extract features from all parts of the input.\n",
        "\n",
        "**c. Types of Padding:**\n",
        "   - **Zero Padding:** Introduces extra pixels with zero values around the input.\n",
        "   - **Valid Padding:** No padding is added.\n",
        "\n",
        "**d. Summary:**\n",
        "   - Padding is essential to prevent information loss at the edges and enhance the network's ability to extract features from the entire input space.\n",
        "\n",
        "---\n",
        "\n",
        "**Q4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size.**\n",
        "\n",
        "**Ans 4:**\n",
        "\n",
        "**a. Zero Padding:**\n",
        "   - **Effect on Feature Map Size:**\n",
        "      - **Increased Size:** Zero padding increases the size of the feature map.\n",
        "   - **Advantage:** It helps preserve information at the edges and maintains the spatial dimensions of the input.\n",
        "\n",
        "**b. Valid Padding:**\n",
        "   - **Effect on Feature Map Size:**\n",
        "      - **Reduced Size:** Valid padding results in a smaller feature map size.\n",
        "   - **Advantage:** It avoids introducing extra pixels and reduces computational load.\n",
        "\n",
        "**c. Comparison:**\n",
        "   - **Common Objective:** Both types aim to maintain the spatial dimensions of the input, albeit through different means.\n",
        "   - **Information Preservation:** Zero padding is more effective in preserving information at the edges.\n",
        "\n",
        "**d. Contrast:**\n",
        "   - **Size Increase:** Zero padding increases the feature map size, while valid padding reduces it.\n",
        "   - **Edge Preservation:** Zero padding explicitly addresses the preservation of information at the edges, which is not a concern with valid padding.\n",
        "\n",
        "**e. Summary:**\n",
        "   - Zero padding increases the feature map size and preserves information at the edges, while valid padding reduces the feature map size and avoids introducing extra pixels. The choice depends on the balance between information preservation and computational efficiency.**"
      ],
      "metadata": {
        "id": "xONZFx0wZRoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LENET"
      ],
      "metadata": {
        "id": "0jkIG5bQZRsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Provide a brief overview of LeNet-5 architecture.**\n",
        "\n",
        "**Ans 1:**\n",
        "\n",
        "**Overview of LeNet-5:**\n",
        "LeNet-5 is a pioneering convolutional neural network (CNN) architecture designed by Yann LeCun and his colleagues in the early 1990s. It was primarily developed for handwritten digit recognition, making it one of the first successful applications of CNNs in the field of computer vision. LeNet-5 played a crucial role in establishing the foundation for modern CNNs and their widespread adoption.\n",
        "\n",
        "**Key Points:**\n",
        "- **Architectural Innovation:** LeNet-5 introduced several architectural innovations, including the use of convolutional layers, subsampling layers (pooling), and fully connected layers.\n",
        "- **Layer Organization:** The architecture consists of several layers, including convolutional layers followed by subsampling (pooling) layers, and finally, fully connected layers for classification.\n",
        "- **Application:** Initially applied to recognize handwritten digits in postal addresses, it showcased the effectiveness of CNNs for image classification.\n",
        "\n",
        "---\n",
        "\n",
        "**Q2. Describe the key components of LeNet-5 and their respective purposes.**\n",
        "\n",
        "**Ans 2:**\n",
        "\n",
        "**a. Convolutional Layers:**\n",
        "   - **Purpose:** Extracts features from the input image using convolutional operations.\n",
        "   - **Operations:** Applies convolutional filters to capture patterns and features in different regions of the input.\n",
        "\n",
        "**b. Subsampling (Pooling) Layers:**\n",
        "   - **Purpose:** Down-samples the feature maps, reducing their spatial dimensions.\n",
        "   - **Operations:** Typically employs max pooling to retain the most significant features.\n",
        "\n",
        "**c. Fully Connected Layers:**\n",
        "   - **Purpose:** Combines the learned features for classification.\n",
        "   - **Operations:** Neurons in these layers are connected to all neurons in the previous layer, allowing for complex feature combinations.\n",
        "\n",
        "**d. Activation Functions:**\n",
        "   - **Purpose:** Introduces non-linearity to the model.\n",
        "   - **Operations:** Typically uses the sigmoid or hyperbolic tangent (tanh) activation functions.\n",
        "\n",
        "**e. LeNet-5 Architecture Overview:**\n",
        "   - **Input Layer:** Accepts the input image.\n",
        "   - **Convolutional Layers:** Extracts local features.\n",
        "   - **Subsampling Layers:** Down-samples the feature maps.\n",
        "   - **Fully Connected Layers:** Perform classification.\n",
        "   - **Output Layer:** Provides the final classification probabilities.\n",
        "\n",
        "**f. Summary:**\n",
        "   - LeNet-5's key components, including convolutional layers, subsampling layers, and fully connected layers, work together to extract hierarchical features and perform image classification.\n",
        "\n",
        "---\n",
        "\n",
        "**Q3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks.**\n",
        "\n",
        "**Ans 3:**\n",
        "\n",
        "**Advantages:**\n",
        "   - **Pioneering Architecture:** LeNet-5 was one of the first successful applications of CNNs, paving the way for future advancements in image classification.\n",
        "   - **Effective Feature Extraction:** The use of convolutional layers allows the network to automatically learn relevant features from input images.\n",
        "   - **Down-Sampling Strategy:** Subsampling layers contribute to spatial down-sampling, enabling the model to focus on essential information and reducing computational complexity.\n",
        "   - **Application Success:** Initially designed for handwritten digit recognition, LeNet-5 demonstrated high accuracy in recognizing digits in postal addresses.\n",
        "\n",
        "**Limitations:**\n",
        "   - **Limited Capacity:** Compared to more modern architectures, LeNet-5 has a limited capacity to capture complex patterns and hierarchies in large and diverse datasets.\n",
        "   - **Not Deep Enough:** With only a few layers, it may struggle with capturing intricate hierarchical features present in more challenging image datasets.\n",
        "   - **Vanishing Gradient Issue:** LeNet-5 may encounter vanishing gradient problems, hindering the training of deep networks.\n",
        "\n",
        "**Overall Impression:**\n",
        "LeNet-5 is a groundbreaking architecture that laid the foundation for CNNs, particularly in image classification. While it may have limitations in handling more complex tasks compared to contemporary architectures, its historical significance and contributions to the field remain undeniable."
      ],
      "metadata": {
        "id": "gQi1v1AsZRxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Implement LeNet-5 using a deep learning framework of your choice e.g.TensorFlow, PyTorch and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide\n",
        "insights**"
      ],
      "metadata": {
        "id": "f5uDcEOUZR1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "U5Uh9gNiaJFD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdMccmHbcNQg",
        "outputId": "ed34c851-d5f4-4374-d014-ab63b525ef29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n"
      ],
      "metadata": {
        "id": "_JSoehEDcNNJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LeNet-5 architecture\n",
        "model = Sequential([\n",
        "  Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "  MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "  Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation=\"relu\"),\n",
        "  MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "  Flatten(),\n",
        "  Dense(120, activation=\"relu\"),\n",
        "  Dense(84, activation=\"relu\"),\n",
        "  Dense(10, activation=\"softmax\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "LFuWbguKcNJy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSIHjh3dbgYa",
        "outputId": "b7ef2e2a-3a81-46cf-de72-b18bc592966a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0819 - accuracy: 0.9747\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0501 - accuracy: 0.9836\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.0375 - accuracy: 0.9884\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0305 - accuracy: 0.9904\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0256 - accuracy: 0.9921\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0218 - accuracy: 0.9929\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 35s 18ms/step - loss: 0.0189 - accuracy: 0.9939\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0174 - accuracy: 0.9945\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0140 - accuracy: 0.9959\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0151 - accuracy: 0.9952\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a573e0f6fb0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OxIPhBzcfZq",
        "outputId": "1406dcda-3259-4645-d8d0-82b95594a979"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0301 - accuracy: 0.9918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUq_44C5cfWP",
        "outputId": "0b55ada2-30eb-4823-b221-21b13190fabd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0300523079931736\n",
            "Test accuracy: 0.9918000102043152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines the LeNet-5 architecture using Keras, preprocesses the MNIST data, and trains the model. Finally, it evaluates the model's performance on the test dataset.\n",
        "\n",
        "Insights\n",
        "Performance:\n",
        "\n",
        "On MNIST, LeNet-5 typically achieves an accuracy of around 99%. This demonstrates its effectiveness for simple image recognition tasks.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and efficient: LeNet-5 has a relatively small number of parameters compared to modern CNNs, making it computationally efficient and suitable for resource-constrained environments.\n",
        "Interpretable: The architecture is relatively straightforward, making it easier to understand the model's behavior and how it makes predictions.\n",
        "Baseline for comparison: Due to its historical significance and simplicity, LeNet-5 serves as a popular baseline model for evaluating the performance of more complex CNNs on similar tasks.\n",
        "Limitations:\n",
        "\n",
        "Limited capacity: LeNet-5 may not be powerful enough to handle more complex image recognition tasks with higher dimensionality and intricate features.\n",
        "Overfitting potential: With a small number of parameters and limited training data, LeNet-5 might be susceptible to overfitting on simpler datasets.\n",
        "Overall:\n",
        "\n",
        "LeNet-5 remains a valuable model in the field of deep learning, despite being developed over two decades ago. Its simplicity, interpretability, and efficiency make it a useful tool for understanding the fundamentals of CNNs and for tackling basic image recognition tasks. While not ideal for complex problems, it serves as a strong foundation and a benchmark for comparing the performance of more advanced models."
      ],
      "metadata": {
        "id": "RHRAUynMdUvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alexnet\n"
      ],
      "metadata": {
        "id": "VweSHwPVabCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Present an overview of the AlexNet architecture.**\n",
        "\n",
        "**Ans 1:**\n",
        "\n",
        "**Overview of AlexNet:**\n",
        "AlexNet is a pioneering convolutional neural network (CNN) architecture designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It gained significant attention by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, demonstrating breakthrough performance in image classification. The architecture consists of eight layers, including five convolutional layers and three fully connected layers.\n",
        "\n",
        "**Key Points:**\n",
        "- **Input Layer:** Accepts input images, typically of size 227x227x3 (RGB).\n",
        "- **Convolutional Layers:** The first five layers are convolutional, extracting hierarchical features.\n",
        "- **Pooling Layers:** Utilizes max-pooling for down-sampling feature maps.\n",
        "- **Fully Connected Layers:** Three fully connected layers for classification.\n",
        "- **Activation Function:** Uses the rectified linear unit (ReLU) activation function.\n",
        "- **Softmax Output:** The final layer applies softmax activation for multi-class classification.\n",
        "\n",
        "---\n",
        "\n",
        "**Q2. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough performance.**\n",
        "\n",
        "**Ans 2:**\n",
        "\n",
        "**a. Parallelization:**\n",
        "   - **Innovation:** AlexNet employed two GPUs, allowing for parallelization and reducing training time.\n",
        "   - **Significance:** This parallelization was crucial for handling the large-scale ImageNet dataset efficiently.\n",
        "\n",
        "**b. ReLU Activation Function:**\n",
        "   - **Innovation:** The use of ReLU activation functions instead of traditional sigmoid or tanh.\n",
        "   - **Significance:** ReLU accelerates convergence during training and mitigates the vanishing gradient problem.\n",
        "\n",
        "**c. Local Response Normalization (LRN):**\n",
        "   - **Innovation:** LRN was applied after some convolutional layers.\n",
        "   - **Significance:** It introduces local competition between neurons, enhancing generalization and promoting inhibitory effects.\n",
        "\n",
        "**d. Overlapping Max Pooling:**\n",
        "   - **Innovation:** Overlapping max pooling with a stride of 2.\n",
        "   - **Significance:** Overlapping pooling reduces spatial resolution, making the network more invariant to translation variations.\n",
        "\n",
        "**e. Data Augmentation:**\n",
        "   - **Innovation:** Extensive data augmentation techniques during training.\n",
        "   - **Significance:** Data augmentation helps prevent overfitting and improves the model's ability to generalize.\n",
        "\n",
        "**f. Dropout:**\n",
        "   - **Innovation:** Dropout was applied to fully connected layers during training.\n",
        "   - **Significance:** Dropout prevents overfitting by randomly dropping units during training, leading to better generalization.\n",
        "\n",
        "**g. Deeper Architecture:**\n",
        "   - **Innovation:** AlexNet had a deeper architecture compared to previous models.\n",
        "   - **Significance:** Increased depth allowed the network to capture more complex features and hierarchical representations.\n",
        "\n",
        "**h. Localized Response Normalization:**\n",
        "   - **Innovation:** Localized Response Normalization was used to enhance contrast between neighboring features.\n",
        "   - **Significance:** Improved model robustness and helped focus on more informative features.\n",
        "\n",
        "---\n",
        "\n",
        "**Q3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet.**\n",
        "\n",
        "**Ans 3:**\n",
        "\n",
        "**a. Convolutional Layers:**\n",
        "   - **Role:** Extract hierarchical features and patterns from input images.\n",
        "   - **Operations:** Convolutional layers use learnable filters to detect low to high-level features like edges, textures, and object parts.\n",
        "\n",
        "**b. Pooling Layers:**\n",
        "   - **Role:** Down-sample feature maps, reducing spatial dimensions.\n",
        "   - **Operations:** AlexNet uses max pooling to retain the most significant features while discarding less important details. Overlapping pooling was employed for translation invariance.\n",
        "\n",
        "**c. Fully Connected Layers:**\n",
        "   - **Role:** Perform classification based on the extracted features.\n",
        "   - **Operations:** Fully connected layers take the flattened output from previous layers and combine learned features to make class predictions.\n",
        "\n",
        "**d. Activation Functions (ReLU):**\n",
        "   - **Role:** Introduce non-linearity to the model.\n",
        "   - **Operations:** ReLU is applied after convolutional and fully connected layers, allowing the network to learn complex mappings efficiently.\n",
        "\n",
        "**e. Local Response Normalization (LRN):**\n",
        "   - **Role:** Enhance generalization by introducing local competition.\n",
        "   - **Operations:** LRN normalizes the responses in a local neighborhood, promoting inhibition and improving the model's ability to discriminate between features.\n",
        "\n",
        "**f. Dropout:**\n",
        "   - **Role:** Prevent overfitting during training.\n",
        "   - **Operations:** Dropout randomly drops units during training, forcing the network to learn more robust and generalizable features.\n",
        "\n",
        "**g. Softmax Output:**\n",
        "   - **Role:** Produce class probabilities for multi-class classification.\n",
        "   - **Operations:** The final layer applies the softmax activation function to convert the model's output into class probabilities.\n",
        "\n",
        "**h. Deeper Architecture:**\n",
        "   - **Role:** Capture more complex features and hierarchical representations.\n",
        "   - **Operations:** A deeper architecture allows the network to learn richer feature representations, leading to improved performance on large-scale image datasets.\n",
        "\n",
        "**i. Summary:**\n",
        "   - Convolutional layers, pooling layers, fully connected layers, and innovative components like ReLU activation, LRN, and dropout collectively contribute to the effectiveness of AlexNet in image classification tasks, leading to its breakthrough performance in the ILSVRC 2012 competition."
      ],
      "metadata": {
        "id": "T3rbSrCjagc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Implement AlexNet using a deep learning framework Of your choice and evaluate its performance\n",
        "on a dataset of your choice.**"
      ],
      "metadata": {
        "id": "lnga7Br_az1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n"
      ],
      "metadata": {
        "id": "GHV2fTIEafxU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n"
      ],
      "metadata": {
        "id": "ALbLsBc3dD6w"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocess data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "\n"
      ],
      "metadata": {
        "id": "Ajidf2ncdD2R"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define AlexNet architecture (modified for MNIST)\n",
        "model = Sequential([\n",
        "  Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "  MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "  Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation=\"relu\"),\n",
        "  MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "  Flatten(),\n",
        "  Dense(128, activation=\"relu\"),\n",
        "  Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "DacYB5ieafNy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoVFcTPhaa4p",
        "outputId": "8af88baa-57e4-4a40-b333-2de011b98774"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.1238 - accuracy: 0.9615\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0401 - accuracy: 0.9872\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0276 - accuracy: 0.9917\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0212 - accuracy: 0.9935\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.0142 - accuracy: 0.9955\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.0112 - accuracy: 0.9965\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0114 - accuracy: 0.9963\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0091 - accuracy: 0.9971\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.0069 - accuracy: 0.9976\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 63s 34ms/step - loss: 0.0069 - accuracy: 0.9977\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7a573e2b7490>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWv0zfX7aa06",
        "outputId": "36807792-f45e-4e72-a5fb-65ec465ceec2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0348 - accuracy: 0.9923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3WYqKZbaaxP",
        "outputId": "1d6a041f-2cbb-494e-97b9-b9106d8edfe5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.03481512889266014\n",
            "Test accuracy: 0.9922999739646912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights:\n",
        "\n",
        "AlexNet is significantly larger and more complex than LeNet-5, designed for more challenging tasks.\n",
        "Modifying the original AlexNet architecture to fit the smaller MNIST dataset involved:\n",
        "Reducing the number of filters and layers.\n",
        "Adjusting kernel sizes and strides for efficient feature extraction.\n",
        "Using smaller dense layers with appropriate activation functions.\n",
        "Despite the modifications, AlexNet still achieves high accuracy on MNIST, exceeding the performance of LeNet-5.\n",
        "This demonstrates the potential of AlexNet's architecture for more complex datasets while revealing the importance of adapting it to specific tasks and data characteristics.\n",
        "Additional considerations:\n",
        "\n",
        "Using dropout layers and regularization techniques can further improve the model's robustness and prevent overfitting.\n",
        "Experimenting with different hyperparameters like learning rate and optimizer settings can potentially enhance its performance.\n",
        "Comparing AlexNet's performance with other CNN architectures on MNIST provides valuable insights into their relative strengths and weaknesses.\n",
        "Overall:\n",
        "\n",
        "Implementing AlexNet with MNIST showcases its capabilities for image recognition tasks beyond its original intended use. While modifications were necessary, the model's adaptability and high performance demonstrate its potential for diverse applications."
      ],
      "metadata": {
        "id": "yqbV4tEzaatT"
      }
    }
  ]
}