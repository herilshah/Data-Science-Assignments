{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa99aea9-a5ad-4451-a3f9-e78c540217a0",
   "metadata": {},
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b7481-a92a-4cce-a5cc-87b13f9d5b2b",
   "metadata": {},
   "source": [
    "Anomaly detection, also known as outlier detection, is the process of identifying data points or patterns that deviate significantly from the norm or expected behavior. The purpose of anomaly detection is to detect and flag unusual or unexpected behavior in a dataset that may indicate a potential problem or opportunity for further investigation. Anomaly detection is used in a wide range of domains, including fraud detection, network intrusion detection, medical diagnosis, and predictive maintenance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2582c7b-405a-4d89-85de-7732ffe5aa90",
   "metadata": {},
   "source": [
    "Q2. What are the key challenges in anomaly detection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90082826-abf0-4749-a28c-45b50a685ded",
   "metadata": {},
   "source": [
    "Anomaly detection poses several key challenges, including:\n",
    "\n",
    "- Lack of labeled data: In many cases, it may be difficult or impossible to obtain labeled data that clearly indicates which data points are anomalous and which are normal.\n",
    "\n",
    "- High dimensionality: Modern datasets often contain a large number of features, making it difficult to identify anomalous behavior in high-dimensional spaces.\n",
    "\n",
    "- Concept drift: The underlying statistical properties of a dataset may change over time, making it difficult to detect anomalies using a fixed set of rules or models.\n",
    "\n",
    "- Class imbalance: Anomalies are often rare events, which can result in class imbalance issues where the majority of data points are normal and only a small fraction are anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567c450-bdfc-4b86-a063-6ef7cf32f7e5",
   "metadata": {},
   "source": [
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad38a3-04ac-462e-9715-3a6ed7ccaf61",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection is a type of anomaly detection that does not require labeled data. Instead, it relies on identifying patterns in the data that deviate significantly from the expected behavior. Unsupervised anomaly detection algorithms may use techniques such as clustering, density estimation, or nearest-neighbor analysis to identify anomalous behavior.\n",
    "\n",
    "Supervisedanomaly detection, on the other hand, requires labeled data that clearly identifies which data points are anomalous and which are not. Supervised anomaly detection algorithms are trained on labeled data and use this training data to identify anomalies in new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d1046-9a11-44f4-bd7f-79d8294eafe5",
   "metadata": {},
   "source": [
    "Q4. What are the main categories of anomaly detection algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1a34d4-cc26-4400-93a8-8259fca5c4a4",
   "metadata": {},
   "source": [
    "Q4. There are several main categories of anomaly detection algorithms:\n",
    "\n",
    "- Statistical methods: These methods use statistical models to identify data points that deviate significantly from the expected behavior. Examples include the z-score method, the Mahalanobis distance method, and the Gaussian mixture model.\n",
    "\n",
    "- Machine learning methods: These methods use machine learning algorithms to identify anomalous behavior in data. Examples include decision trees, neural networks, and support vector machines.\n",
    "\n",
    "- Clustering-based methods: These methods group similar data points together and identify data points that do not belong to any cluster as anomalies. Examples include k-means clustering and density-based clustering.\n",
    "\n",
    "- Distance-based methods: These methods calculate the distance between data points and identify data points that are farthest from the rest of the data as anomalies. Examples include the k-nearest neighbor algorithm and the local outlier factor.\n",
    "\n",
    "- Information-theoretic methods: These methods use information theory to identify unusual patterns in data. Examples include the minimum description length method and the Kolmogorov complexity method.\n",
    "\n",
    "The choice of algorithm will depend on the specific application and the characteristics of the data being analyzed. Each algorithm has its own strengths and weaknesses, and selecting the appropriate algorithm is crucial for achieving accurate and reliable anomaly detection results.Regarding your follow-up question, class imbalance occurs when one class in a dataset has significantly fewer instances than the other class(es). In the context of anomaly detection, the normal class is typically much larger than the anomalous class(es). This can pose a challenge to anomaly detection algorithms because they are often designed to optimize overall accuracy, which may result in a bias towards the majority class.\n",
    "\n",
    "In the case of class imbalance, the performance of the anomaly detection algorithm may be evaluated using metrics such as precision, recall, and F1-score, which take into account the number of true positives, false positives, and false negatives. For example, in the case of fraud detection, a high precision score would indicate that the algorithm is correctly identifying a high percentage of fraudulent transactions, while a high recall score would indicate that the algorithm is correctly identifying a high percentage of all fraudulent transactions in the dataset, regardless of false positives.\n",
    "\n",
    "To address the issue of class imbalance, several techniques can be used, such as:\n",
    "\n",
    "- Resampling: This involves either oversampling the minority class or undersampling the majority class to balance the class distribution.\n",
    "\n",
    "- Cost-sensitive learning: This involves assigning different costs to misclassification errors for different classes, such that misclassifying an anomaly incurs a higher cost than misclassifying a normal data point.\n",
    "\n",
    "- Ensemble methods: This involves combining multiple anomaly detection algorithms to improve overall performance and reduce the impact of class imbalance.\n",
    "\n",
    "Overall, addressing class imbalance is an important consideration in anomaly detection, and selecting anappropriate approach for dealing with class imbalance can help improve the accuracy and reliability of anomaly detection algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ac6b5-cb2e-4392-864d-2fd203d137c0",
   "metadata": {},
   "source": [
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f5b7d-ebb0-4d17-8a3a-f67c22183750",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods make several assumptions about the data, including:\n",
    "\n",
    "- Normal behavior is characterized by a dense region in the feature space, while anomalous behavior is characterized by sparse regions.\n",
    "\n",
    "- Anomalies are located far away from the dense region of normal behavior.\n",
    "\n",
    "- The density of normal data points decreases smoothly as we move away from the dense region.\n",
    "\n",
    "- Anomalies are few in number compared to the normal data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68860f62-fb85-4237-a0c1-d018ea2df66f",
   "metadata": {},
   "source": [
    "Q6. How does the LOF algorithm compute anomaly scores?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc08c8-7111-4e59-99bc-e510d8a96ecb",
   "metadata": {},
   "source": [
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores by comparing the local density of a data point with the local densities of its neighbors. Specifically, the algorithm computes the average local density of a data point's k-nearest neighbors, and then divides the local density of the data point by this average. The resulting score is a measure of how much more or less dense the data point is compared to its neighbors, with scores greater than 1 indicating that the data point is more sparse and therefore more likely to be an anomaly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888cdd88-f909-44e1-b624-e67b1ae640a8",
   "metadata": {},
   "source": [
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e3659-768f-4e75-92c7-3a2e245585dd",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm has several key parameters, including:\n",
    "\n",
    "- n_estimators: The number of decision trees to include in the forest.\n",
    "\n",
    "- max_samples: The number of samples to draw from the dataset to build each decision tree.\n",
    "\n",
    "- max_depth: The maximum depth of each decision tree.\n",
    "\n",
    "- contamination: The expected percentage of anomalies in the dataset.\n",
    "\n",
    "- bootstrap: Whether to use bootstrap sampling to select the samples for each decision tree.\n",
    "\n",
    "- random_state: A random seed value forreproducibility.\n",
    "\n",
    "The n_estimators parameter controls the number of trees in the forest and affects the algorithm's performance and memory usage. The max_samples parameter controls the number of samples used to build each tree and affects the diversity of the trees in the forest. The max_depth parameter controls the maximum depth of each tree and affects the granularity of the anomaly scores. The contamination parameter sets the expected percentage of anomalies in the dataset, which can help the algorithm adjust the threshold for labeling a data point as an anomaly. The bootstrap parameter controls whether to use bootstrap sampling to select the samples for each tree, which can help improve the diversity of the trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab886de7-f2c1-41ab-ba54-ec7174dd1f2d",
   "metadata": {},
   "source": [
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe67e18-b4ca-44c4-a791-53e8078a5c3a",
   "metadata": {},
   "source": [
    "To compute the anomaly score of a data point using K-nearest neighbors (KNN) with K=10, we need to identify the distance between the data point and its 10th nearest neighbor. If the distance is large, the data point is likely to be an anomaly.\n",
    "In this case, the data point has only 2 neighbors of the same class within a radius of 0.5. Since K=10, we need to find the distance between the data point and its 10th nearest neighbor. If the data point has only 2 neighbors within a radius of 0.5, it is unlikely that it will have 10 neighbors within the same radius. Therefore, we cannot compute the anomaly score of the data point using KNN with K=10.\n",
    "However, if we still want to compute the anomaly score using KNN with K=10, we can extend the distance radius until we find 10 neighbors. For example, if we extend the radius to 1, we may find 10 neighbors. We can then compute the distance between the data point and its 10th nearest neighbor and use it to compute the anomaly score. The larger the distance, the higher the anomaly score.\n",
    "Anomaly Score = 1 / (average distance to k nearest neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d8dab5-df9c-476f-a258-dcc8285fb83c",
   "metadata": {},
   "source": [
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1693781a-95d3-4b1b-8cd8-c54aa87fba25",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm generates a forest of decision trees, where each data point is isolated in a different partition of the feature space. The anomaly score of a data point is computed based on the average path length of the data point in the trees of the forest.\n",
    "If a data point has an average path length of 5.0 compared to the average path length of the trees, we can compute its anomaly score using the following formula:\n",
    "Anomaly Score = 2^(-average path length / c(n))\n",
    "where c(n) is a constant that depends on the number of data points n in the dataset. The value of c(n) can be computed as:\n",
    "c(n) = 2 * H(n-1) - (2 * (n-1) / n)\n",
    "where H(n-1) is the harmonic number of n-1.\n",
    "For a dataset of 3000 data points, c(n) can be computed as:\n",
    "c(3000) = 2 * H(2999) - (2 * 2999 / 3000) = 11.8979\n",
    "Using this value of c(n), we can compute the anomaly score of the data point with an average path length of 5.0 as:\n",
    "Anomaly Score = 2^(-5.0 / 11.8979) = 0.5017\n",
    "This indicates that the data point is less anomalous than a data point with an average path length that is farther from the average path length of the trees.\n",
    "Below is manual method to calulate anomaly score in python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a12a8452-6039-4540-8b7a-c7e089a8c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The anomaly score of the data point is 0.7809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Generate a dataset of 3000 data points with 10 features\n",
    "X = np.random.randn(3000, 10)\n",
    "\n",
    "# Fit an Isolation Forest model with 100 trees\n",
    "clf = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "clf.fit(X)\n",
    "\n",
    "# Compute the average path length of a data point with 5.0 compared to the average path length of the trees\n",
    "avg_path_length = 5.0\n",
    "tree_path_lengths = np.zeros(clf.n_estimators)\n",
    "for i, tree in enumerate(clf.estimators_):\n",
    "    path = tree.decision_path(X)\n",
    "    tree_path_lengths[i] = path.indices.size - 1\n",
    "avg_tree_path_length = np.mean(tree_path_lengths)\n",
    "\n",
    "# Compute the anomaly score using the formula for Isolation Forest\n",
    "c = 2 * np.log(X.shape[0] - 1) - (2 * (X.shape[0] - 1) / X.shape[0])\n",
    "anomaly_score = 2 ** (-avg_path_length / c) \n",
    "\n",
    "print(f\"The anomaly score of the data point is {anomaly_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d065b0-aa2f-4285-a1f9-49e7eec3da9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.39939803 -0.43744359 -0.43014875 ... -0.46469931 -0.44142911\n",
      " -0.4077563 ]\n",
      "\n",
      "The mean anomaly score is -0.4381\n"
     ]
    }
   ],
   "source": [
    "#Below is sklearn score_samples method to find anomaly_scores\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Generate a dataset of 3000 data points with 10 features\n",
    "X = np.random.randn(3000, 10)\n",
    "\n",
    "# Fit an Isolation Forest model with 100 trees\n",
    "clf = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "clf.fit(X)\n",
    "\n",
    "# Compute the anomaly scores for the data points\n",
    "anomaly_scores = clf.score_samples(X)\n",
    "\n",
    "# Print the anomaly scores\n",
    "print(anomaly_scores)\n",
    "\n",
    "\n",
    "# Compute the mean of the anomaly scores\n",
    "mean_anomaly_score = np.mean(anomaly_scores)\n",
    "\n",
    "# Print the mean anomaly score\n",
    "print(f\"\\nThe mean anomaly score is {mean_anomaly_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5e0b1-94bf-4ec4-af68-e88167a408be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
