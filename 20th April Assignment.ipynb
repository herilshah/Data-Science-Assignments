{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f44cd22-d95c-4a3f-a3fc-aecc85b2a0d9",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b5439-5ed7-466b-98c3-4a23a0953723",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric algorithm that makes predictions based on the similarity of a new instance to its k nearest neighbors in the training data. In KNN, the \"k\" refers to the number of nearest neighbors considered for classification or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d077d69-360b-4778-98cb-75c96528c0e0",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8ee21-2a8e-454a-96cd-3143e4312e5b",
   "metadata": {},
   "source": [
    "The value of K in KNN is typically chosen through a process called model selection. The optimal value of K depends on the dataset and the problem at hand. Choosing a small value of K (e.g., K=1) can lead to an unstable and noisy decision boundary, while choosing a large value of K may smooth out the decision boundary and introduce bias. Common techniques for choosing the value of K include cross-validation, grid search, and domain knowledge. It is important to select a value of K that balances between overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39ea59-d21e-4121-a793-4e0992c17f65",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159671cd-f790-4eb7-be62-4bbdae0bb906",
   "metadata": {},
   "source": [
    "KNN Classifier: KNN classifier is used for classification tasks. It assigns a class label to a new instance based on the majority class among its k nearest neighbors. The predicted output is a discrete class label.\n",
    "\n",
    "KNN Regressor: KNN regressor is used for regression tasks. It predicts a continuous numerical value for a new instance based on the average (or weighted average) of the target values of its k nearest neighbors. The predicted output is a continuous value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177236d5-3595-4b52-9973-3b52cbd3996f",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1cc008-e7f1-4dbd-8e1b-5a3ed90cc068",
   "metadata": {},
   "source": [
    "The performance of KNN can be measured using various evaluation metrics, depending on the specific task:\n",
    "\n",
    "Classification: Accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (ROC-AUC) are commonly used metrics for evaluating the classification performance of KNN.\n",
    "\n",
    "Regression: Mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared (coefficient of determination) are commonly used metrics for evaluating the regression performance of KNN.\n",
    "It is important to choose the appropriate metric based on the problem and the desired evaluation criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04edf91-092e-4e0f-9580-a5ec6221c8a9",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ecef4-0c62-4a0a-a281-6951dc458964",
   "metadata": {},
   "source": [
    "The curse of dimensionality in KNN refers to the degradation of algorithm performance as the number of input features (dimensions) increases. As the number of dimensions increases, the Euclidean distance between data points becomes less meaningful, and the data becomes more sparse. In high-dimensional spaces, the majority of data points may be far away from each other, resulting in less reliable nearest neighbors. This can lead to decreased accuracy and increased computational complexity in KNN. To mitigate the curse of dimensionality, dimensionality reduction techniques or feature selection methods can be applied to reduce the number of irrelevant or redundant features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417a005-5764-42d2-8fd3-f936c5f0ba6a",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6454209-6a53-45e0-9868-3c743aa30b4d",
   "metadata": {},
   "source": [
    "Handling missing values in KNN requires imputation, which is the process of filling in the missing values with estimated values. There are several approaches to handle missing values in KNN:\n",
    "\n",
    "Deleting instances: If the dataset has a small number of missing values, you can remove the instances with missing values. However, this approach may result in a loss of valuable information if the instances contain other important features.\n",
    "\n",
    "Filling with a constant value: You can replace the missing values with a constant value such as zero or the mean/median value of the feature. This approach assumes that the missing values are missing completely at random and does not take into account the relationships between features.\n",
    "\n",
    "KNN-based imputation: In this approach, you can use the KNN algorithm itself to estimate missing values. For each instance with missing values, you can find its k nearest neighbors based on the available features and use the average (or weighted average) of their values to impute the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe19be-ea53-4303-98ec-1072b37bb4f8",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb1aa7-0aaf-4411-ace1-e9a0d3966dc0",
   "metadata": {},
   "source": [
    "The performance of KNN classifier and regressor depends on the specific problem and the nature of the data. Here are some comparisons between the two:\n",
    "\n",
    "KNN Classifier: KNN classifier is suitable for problems where the output is a discrete class label. It works well when the decision boundary is non-linear and the classes have distinct regions in the feature space. KNN classifier can handle multi-class classification tasks effectively.\n",
    "\n",
    "KNN Regressor: KNN regressor is suitable for problems where the output is a continuous numerical value. It can capture non-linear relationships between the features and the target variable. KNN regressor works well when there is a smooth transition between neighboring data points in the feature space.\n",
    "\n",
    "The choice between KNN classifier and regressor depends on the problem at hand. If the target variable is categorical, such as predicting the type of a flower, KNN classifier is appropriate. If the target variable is continuous, such as predicting the price of a house, KNN regressor is more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05995a-dea2-4158-97b3-e6cc3812df12",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a287b9c-3d6a-4084-80f9-94e791267b48",
   "metadata": {},
   "source": [
    "Strengths of KNN algorithm:\n",
    "\n",
    "1.Simple and intuitive algorithm.\n",
    "2.Non-parametric nature, which allows it to capture complex relationships.\n",
    "3.Can handle multi-class classification and regression tasks.\n",
    "4.Robust to outliers and noisy data.\n",
    "\n",
    "Weaknesses of KNN algorithm:\n",
    "\n",
    "1.Computationally expensive, especially with large datasets.\n",
    "2.Sensitive to the choice of k and distance metric.\n",
    "3.Requires feature scaling for accurate distance calculations.\n",
    "4.Curse of dimensionality can affect its performance.\n",
    "\n",
    "To address these weaknesses, some techniques can be employed, such as using efficient data structures (e.g., KD-trees) for faster nearest neighbor search, selecting an appropriate value of k through model selection techniques, applying dimensionality reduction methods, and performing feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c1bb5-3e09-4c97-a6e0-117c5cd4b36d",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1ecd0-fb5e-4976-9203-fda1979a1150",
   "metadata": {},
   "source": [
    "The difference between Euclidean distance and Manhattan distance lies in the way they measure distance between two points in KNN:\n",
    "\n",
    "Euclidean distance: Euclidean distance is the straight-line distance between two points in the feature space. It is calculated as the square root of the sum of squared differences between the corresponding feature values. Euclidean distance considers the magnitude of differences in all dimensions.\n",
    "\n",
    "Manhattan distance: Manhattan distance, also known as city block distance or L1 distance, is the sum of absolute differences between the corresponding feature values. It measures the distance between two points by summing the absolute differences along each dimension. Manhattan distance considers only the magnitude of differences along each dimension.\n",
    "\n",
    "The choice between Euclidean distance and Manhattan distance depends on the problem and the nature of the data. Euclidean distance is commonly used when the feature values have continuous and comparable scales. Manhattan distance is more suitable when dealing with features that are not on the same scale or when the differences along each dimension are more meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da247b2-2b9c-4be3-9e60-c597764a6129",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f5845-8a4e-4573-9350-bb67679ba873",
   "metadata": {},
   "source": [
    "Feature scaling is important in KNN because it ensures that all features contribute equally to the distance calculations. Since KNN relies on measuring the distances between data points, if the features have different scales or units, it can lead to biased distance calculations. Features with larger scales can dominate the distance metric and overshadow the contributions of other features.\n",
    "\n",
    "To address this, it is common to apply feature scaling techniques such as normalization or standardization. Normalization scales the features to a range of [0, 1], while standardization transforms the features to have zero mean and unit variance. By scaling the features, they are placed on a similar scale, and each feature contributes proportionally to the distance calculations, resulting in a more balanced representation of the data. Feature scaling ensures that all features are equally considered in the KNN algorithm and prevents any feature from having a disproportionate influence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
